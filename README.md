# Legal Assistant (Chat + Documentos)

MVP de um assistente jur√≠dico brasileiro com dois modos:

1. **Consulta** (RAG + cita√ß√µes)
2. **Gera√ß√£o de Documentos** (form ‚Üí template .docx)

## üöÄ Modo de Conversa Multi-turn

O endpoint `/chat` agora suporta conversas persistentes em mem√≥ria. Cada requisi√ß√£o pode conter um `conversation_id` para continuar o hist√≥rico.

### Request

```bash
POST /chat
{
  "conversation_id": "<opcional>",
  "message": "Qual √© o objetivo da recupera√ß√£o judicial?",
  "use_llm": true,
  "k": 12,
  "max_history": 8
}
```

Se `conversation_id` n√£o for enviado, o backend cria um novo e retorna no payload.

### Response

```json
{
  "answer": "...",
  "citations": ["Lei 11.101 art. 47", "Lei 11.101 art. 51"],
  "conversation_id": "b2f1e7c8d8e94b7a9d4c1e0d4c2f8a7b",
  "messages": [
    {"role": "user", "content": "Qual √© o objetivo da recupera√ß√£o judicial?"},
    {"role": "assistant", "content": "..."}
  ]
}
```

### Endpoints auxiliares

```bash
GET /conversation/{conversation_id}
POST /conversation/{conversation_id}/reset
```

### Estrat√©gia de Hist√≥rico

O motor de busca considera as √∫ltimas `max_history` mensagens do usu√°rio para criar uma consulta combinada. O hist√≥rico completo √© mantido at√© 50 mensagens (limite configurado em mem√≥ria).

Para produzir uma conversa de verdade no frontend, basta reutilizar o `conversation_id` retornado e exibir o array `messages` em formato de chat.

### Futuras Melhorias

- Persist√™ncia em Redis ou banco (atualmente somente mem√≥ria local)
- Resumo autom√°tico (message windowing) para conversas longas
- Streaming de tokens via Server-Sent Events ou WebSocket
- Controles de custo/token

## Como rodar (dev)

```bash
python -m venv .venv
".venv\\Scripts\\activate"  # PowerShell Windows
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

## Endpoints Principais

```text
POST /chat
POST /documents/peticao-inicial-cobranca
GET  /conversation/{conversation_id}
POST /conversation/{conversation_id}/reset
```

## Estrutura

```text
app/
  main.py                # FastAPI + rotas
  rag.py                 # stub de RAG
  prompts/               # prompts base e processamento
  documents/             # gera√ß√£o de documentos
scripts/                 # ingest√£o e indexa√ß√£o
data/                    # dados legais (raw/processed)
```

## Observa√ß√µes

- Este projeto √© **esqueleto**: os m√©todos em `rag.py` est√£o com stubs para voc√™ ligar ao seu √≠ndice vetorial (Qdrant / pgvector / Pinecone). Agora o fluxo local usa SentenceTransformers + Qdrant.
- Para gera√ß√£o de documentos personalize templates em `app/documents/templates/`.
- Ajuste vari√°veis de ambiente (ex: `USE_OLLAMA=true`).
- Avalie requisitos de LGPD para armazenamento de hist√≥rico.

## üì¶ Stack Local (Embeddings + Rerank)

Fluxo t√≠pico para adicionar uma nova lei usando somente recursos locais:

```bash
# 1. Ingest√£o (gera JSONL processado de artigos/chunks)
python -m scripts.ingest --lei "11.101/2005" --input data/raw/lei_11101_2005.txt

# (Opcional) baixar via URL oficial:
python -m scripts.ingest --lei "11.101/2005" --url "https://www.planalto.gov.br/..." --output data/processed/lei_11101_2005.jsonl --raw-html-out data/raw/lei_11101_2005.html

# 2. Indexa√ß√£o local (embeddings CPU)
python -m scripts.index_qdrant_local --jsonl data/processed/lei_11101_2005.jsonl --collection leis --recreate

# 3. Busca vetorial simples
python -m scripts.search_qdrant_local --query "plano de recupera√ß√£o judicial" --k 8

# 4. Busca + rerank (melhor precis√£o)
python -m scripts.search_qdrant_local --query "plano de recupera√ß√£o judicial" --k 12 --n 5 --rerank
```

Vari√°veis √∫teis:

```text
EMBED_MODEL=intfloat/multilingual-e5-base   # mudar modelo de embeddings
RERANK_MODEL=BAAI/bge-reranker-v2-m3        # mudar modelo cross-encoder
QDRANT_COLLECTION=leis                      # nome da collection
QDRANT_HOST=localhost QDRANT_PORT=6333      # endpoint Qdrant
```

Para ver todos os resultados antes do rerank final: `--show-all`.
