# Legal Assistant (Chat + Documentos)

MVP de um assistente jur√≠dico brasileiro com dois modos:

1. **Consulta** (RAG + cita√ß√µes)
2. **Gera√ß√£o de Documentos** (form ‚Üí template .docx)

## üöÄ Modo de Conversa Multi-turn

O endpoint `/chat` agora suporta conversas persistentes em mem√≥ria. Cada requisi√ß√£o pode conter um `conversation_id` para continuar o hist√≥rico.

### Request

```bash
POST /chat
{
  "conversation_id": "<opcional>",
  "message": "Qual √© o objetivo da recupera√ß√£o judicial?",
  "use_llm": true,
  "k": 12,
  "max_history": 8
}
```

Se `conversation_id` n√£o for enviado, o backend cria um novo e retorna no payload.

### Response

```json
{
  "answer": "...",
  "citations": ["Lei 11.101 art. 47", "Lei 11.101 art. 51"],
  "conversation_id": "b2f1e7c8d8e94b7a9d4c1e0d4c2f8a7b",
  "messages": [
    {"role": "user", "content": "Qual √© o objetivo da recupera√ß√£o judicial?"},
    {"role": "assistant", "content": "..."}
  ]
}
```

### Endpoints auxiliares

```bash
GET /conversation/{conversation_id}
POST /conversation/{conversation_id}/reset
```

### Estrat√©gia de Hist√≥rico

O motor de busca considera as √∫ltimas `max_history` mensagens do usu√°rio para criar uma consulta combinada. O hist√≥rico completo √© mantido at√© 50 mensagens (limite configurado em mem√≥ria).

Para produzir uma conversa de verdade no frontend, basta reutilizar o `conversation_id` retornado e exibir o array `messages` em formato de chat.

### Futuras Melhorias

- Persist√™ncia em Redis ou banco (atualmente somente mem√≥ria local)
- Resumo autom√°tico (message windowing) para conversas longas
- Streaming de tokens via Server-Sent Events ou WebSocket
- Controles de custo/token

## Como rodar (dev)

```bash
python -m venv .venv
".venv\\Scripts\\activate"  # PowerShell Windows
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

## Endpoints Principais

```text
POST /chat
POST /documents/peticao-inicial-cobranca
GET  /conversation/{conversation_id}
POST /conversation/{conversation_id}/reset
```

## Estrutura

```text
app/
  main.py                # FastAPI + rotas
  rag.py                 # stub de RAG
  prompts/               # prompts base e processamento
  documents/             # gera√ß√£o de documentos
scripts/                 # ingest√£o e indexa√ß√£o
data/                    # dados legais (raw/processed)
```

## Observa√ß√µes

- Este projeto √© **esqueleto**: os m√©todos em `rag.py` est√£o com stubs para voc√™ ligar ao seu √≠ndice vetorial (Qdrant / pgvector / Pinecone). Agora o fluxo local usa SentenceTransformers + Qdrant.
- Para gera√ß√£o de documentos personalize templates em `app/documents/templates/`.
- Ajuste vari√°veis de ambiente (ex: `USE_OLLAMA=true`).
- Avalie requisitos de LGPD para armazenamento de hist√≥rico.

## üì¶ Stack Local (Embeddings + Rerank)

Fluxo t√≠pico para adicionar uma nova lei usando somente recursos locais:

```bash
# 1. Ingest√£o (gera JSONL processado de artigos/chunks)
python -m scripts.ingest --lei "11.101/2005" --input data/raw/lei_11101_2005.txt

# (Opcional) baixar via URL oficial:
python -m scripts.ingest --lei "11.101/2005" --url "https://www.planalto.gov.br/..." --output data/processed/lei_11101_2005.jsonl --raw-html-out data/raw/lei_11101_2005.html

# 2. Indexa√ß√£o local (embeddings CPU)
python -m scripts.index_qdrant_local --jsonl data/processed/lei_11101_2005.jsonl --collection leis --recreate

# 3. Busca vetorial simples
python -m scripts.search_qdrant_local --query "plano de recupera√ß√£o judicial" --k 8

# 4. Busca + rerank (melhor precis√£o)
python -m scripts.search_qdrant_local --query "plano de recupera√ß√£o judicial" --k 12 --n 5 --rerank
```

Vari√°veis √∫teis:

```text
EMBED_MODEL=intfloat/multilingual-e5-base   # mudar modelo de embeddings
RERANK_MODEL=BAAI/bge-reranker-v2-m3        # mudar modelo cross-encoder
QDRANT_COLLECTION=leis                      # nome da collection
QDRANT_HOST=localhost QDRANT_PORT=6333      # endpoint Qdrant
```

Para ver todos os resultados antes do rerank final: `--show-all`.

## üß† Gera√ß√£o de Documento com IA (Peti√ß√£o Inicial de Cobran√ßa)

Al√©m de preencher manualmente os campos do JSON para o endpoint de documento, voc√™ pode gerar se√ß√µes automaticamente (fatos, pedidos, provas) usando recupera√ß√£o + LLM local (Ollama).

### Pr√©-requisitos

1. Qdrant rodando local e j√° indexado (ver se√ß√£o anterior).
2. Ollama instalado e modelo carregado (ex.: `ollama pull llama3.1:8b`).
3. Vari√°veis de ambiente (opcionais):
   - `OLLAMA_HOST` (default `http://localhost:11434`)
   - `OLLAMA_MODEL` (ex.: `llama3.1:8b`)

### Fun√ß√£o Python

A fun√ß√£o `generate_peticao_inicial_cobranca_ai` em `app/documents/generator.py`:

- Recupera artigos relevantes da cole√ß√£o (embeddings locais)
- Gera texto estruturado para fatos / pedidos / provas se estiverem vazios ou se `force=True`
- Renderiza o template `.docx` final

```python
from app.documents.generator import generate_peticao_inicial_cobranca_ai

entrada = {
  "foro": "Foro Central da Comarca X",
  "autor": {"nome": "Jo√£o Silva", "cpf": "123.456.789-00", "endereco": "Rua A, 100"},
  "reu": {"nome": "Empresa Y Ltda.", "cnpj": "12.345.678/0001-99", "endereco": "Av. B, 200"},
  "valor_causa": 15000.00,
  # campos fatos/pedidos/provas vazios => ser√£o gerados
}

doc_path = generate_peticao_inicial_cobranca_ai(
  entrada,
  consulta_caso="Cliente n√£o recebeu valores de contrato de presta√ß√£o de servi√ßos firmado em 2023.",
  k=12,            # n√∫mero de chunks recuperados
  n_context=6,     # reservado para futura l√≥gica (rerank)
  force=False      # True para sobrescrever se j√° houver conte√∫do
)
print("Gerado:", doc_path)
```

### Como funciona internamente

1. Normaliza a descri√ß√£o do caso com `preprocess_question`.
2. Busca vetorial em Qdrant (`k` resultados).
3. Monta o CONTEXTO concatenando trechos (truncados para ~900 chars cada).
4. Chama o Ollama usando um prompt jur√≠dico padronizado (cita artigos se poss√≠vel).
5. Para `pedidos` e `provas`, transforma a resposta em lista de itens por linha.
6. Renderiza docx final com `docxtpl`.

### Dicas de Prompt

- Forne√ßa contexto factual claro em `consulta_caso`.
- Ajuste `k` se vier pouco fundamento legal (maior recall).
- Se a sa√≠da vier prolixa, considere reduzir o modelo ou p√≥s-processar (ex.: limitar n√∫mero de linhas em pedidos).

### Poss√≠veis Extens√µes

- Reranqueamento dos artigos antes da gera√ß√£o (usar `rerank_local.py`).
- Gera√ß√£o de fundamenta√ß√£o jur√≠dica e jurisprud√™ncia em se√ß√µes separadas.
- Verifica√ß√£o autom√°tica de cita√ß√µes legais (regex para "Art.").
