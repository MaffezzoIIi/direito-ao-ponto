# Direito ao Ponto

O projeto buscar desenvolver um sistema de interpretraÃ§Ã£o de linguagem natural e responder ao usuÃ¡rios com respostas objetivas e concretas sobre assuntos jurÃ­dicos, evitanto alucinaÃ§Ãµes e sempre se baseando em uma arquitetura modular voltada tanto para um serviÃ§o on-premisse ou um serviÃ§o em nuvem.

Para a sua utilizaÃ§Ã£o sÃ£o necessÃ¡rios trÃªs repositÃ³rios atual, onde o presente repositÃ³rio se resume em ser a ponte entre os outros dois serviÃ§os, o frontend para interface com usuÃ¡rio e o history que salva o registro das conversas para utilizar no contexto das conversas e devolver respostas que faÃ§Ã£o sentido no contexto atual da conversa.

1. [RepositÃ³rio resonpsÃ¡vel pelas conversas;](https://github.com/MaffezzoIIi/legal-assistant-history)
2. [RepositÃ³rio do frontend](https://github.com/MaffezzoIIi/direito-ao-ponto-frontend)

## ğŸ›ï¸ Tratamento dos dados jurÃ­dicos

O tratamento dos dados jurÃ­dicos Ã© realizado por meio de uma sequÃªncia de scripts que processam, indexam e permitem a busca eficiente sobre textos legais. O fluxo tÃ­pico envolve:

1. **IngestÃ£o dos dados:** O script `ingest.py` lÃª arquivos brutos de leis (TXT ou HTML) e os transforma em arquivos JSONL estruturados, segmentando os textos em artigos ou trechos relevantes.

2. **IndexaÃ§Ã£o dos dados:** O script `index_qdrant_local.py` consome o arquivo JSONL gerado e realiza a indexaÃ§Ã£o dos textos em um banco vetorial (Qdrant), utilizando embeddings para facilitar buscas semÃ¢nticas.

3. **Busca vetorial:** O script `search_qdrant_local.py` permite realizar buscas semÃ¢nticas sobre os dados indexados, retornando os trechos mais relevantes para uma consulta jurÃ­dica.

4. **Reranqueamento (opcional):** Para maior precisÃ£o, o script `rerank_local.py` pode ser utilizado para reranquear os resultados da busca, usando modelos de cross-encoder.

> [Qdrant](https://qdrant.tech/)  
> [Vector database](https://en.wikipedia.org/wiki/Vector_database)

Diagrama de SequÃªncia (Mermaid)

```mermaid
sequenceDiagram
    participant UsuÃ¡rio
    participant ingest.py
    participant index_qdrant_local.py
    participant Qdrant
    participant search_qdrant_local.py
    participant rerank_local.py

    UsuÃ¡rio->>ingest.py: Fornece arquivo de lei (TXT/HTML)
    ingest.py->>ingest.py: Processa e segmenta artigos
    ingest.py->>UsuÃ¡rio: Gera arquivo JSONL estruturado

    UsuÃ¡rio->>index_qdrant_local.py: Inicia indexaÃ§Ã£o
    index_qdrant_local.py->>Qdrant: Indexa artigos com embeddings

    UsuÃ¡rio->>search_qdrant_local.py: Realiza consulta jurÃ­dica
    search_qdrant_local.py->>Qdrant: Busca vetorial
    Qdrant->>search_qdrant_local.py: Retorna trechos relevantes

    search_qdrant_local.py->>rerank_local.py: (Opcional) Reranqueia resultados
    rerank_local.py->>search_qdrant_local.py: Retorna resultados reranqueados

    search_qdrant_local.py->>UsuÃ¡rio: Exibe resultados finais
```

## ğŸ„â€â™‚ï¸ UtilizaÃ§Ã£o do RAG + Ollama (LLM)

O sistema utiliza uma abordagem RAG (Retrieval-Augmented Generation) combinada com um modelo de linguagem local (Ollama) para gerar respostas jurÃ­dicas fundamentadas e contextualizadas. O fluxo consiste em:

1. **RecepÃ§Ã£o da pergunta do usuÃ¡rio**
2. **RecuperaÃ§Ã£o de contexto**: Busca vetorial em Qdrant para encontrar trechos legais relevantes Ã  consulta
3. **Reranqueamento**: (opcional) Melhora a precisÃ£o dos resultados usando modelo cross-encoder
4. **ConstruÃ§Ã£o do prompt**: Os trechos recuperados sÃ£o formatados e inseridos em um prompt jurÃ­dico
5. **GeraÃ§Ã£o da resposta**: O prompt Ã© enviado ao modelo LLM (Ollama), que gera uma resposta fundamentada
6. **Retorno ao usuÃ¡rio**: A resposta Ã© entregue junto com as citaÃ§Ãµes legais utilizadas

### Diagrama de Arquitetura (Mermaid)

```mermaid
flowchart TD
  A[UsuÃ¡rio] -->|Pergunta| B[API /chat]
  B --> C[Busca vetorial Qdrant]
  C --> D["Rerank (opcional)"]
  D --> E[ConstruÃ§Ã£o do Prompt]
  E --> F["LLM (Ollama)"]
  F --> G[Resposta fundamentada]
  G -->|Exibe| A
  C -.-> E
  C -.-> G
```

### Exemplo de fluxo

1. UsuÃ¡rio envia uma pergunta jurÃ­dica via endpoint `/chat`.
2. O backend recupera trechos relevantes usando busca vetorial.
3. (Opcional) Aplica rerank para priorizar os melhores trechos.
4. Monta um prompt com o contexto legal e a pergunta do usuÃ¡rio.
5. O Ollama gera uma resposta fundamentada, citando os artigos recuperados.
6. O sistema retorna a resposta e as citaÃ§Ãµes ao usuÃ¡rio.

Esse fluxo garante que as respostas sejam sempre baseadas em fontes legais indexadas, reduzindo alucinaÃ§Ãµes e aumentando a confiabilidade do sistema.

> [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)  

## ğŸš€ Modo de Conversa Multi-turn

O endpoint `/chat` agora suporta conversas persistentes em memÃ³ria. Cada requisiÃ§Ã£o pode conter um `conversation_id` para continuar o histÃ³rico.

### Request

```bash
POST /chat
{
  "conversation_id": "<opcional>",
  "message": "Qual Ã© o objetivo da recuperaÃ§Ã£o judicial?",
  "use_llm": true,
  "k": 12,
  "max_history": 8
}
```

Se `conversation_id` nÃ£o for enviado, o backend cria um novo e retorna no payload.

### Response

```json
{
  "answer": "...",
  "citations": ["Lei 11.101 art. 47", "Lei 11.101 art. 51"],
  "conversation_id": "b2f1e7c8d8e94b7a9d4c1e0d4c2f8a7b",
  "messages": [
    {"role": "user", "content": "Qual Ã© o objetivo da recuperaÃ§Ã£o judicial?"},
    {"role": "assistant", "content": "..."}
  ]
}
```

### EstratÃ©gia de HistÃ³rico

O motor de busca considera as Ãºltimas `max_history` mensagens do usuÃ¡rio para criar uma consulta combinada. O histÃ³rico completo Ã© mantido atÃ© 50 mensagens (limite configurado em memÃ³ria).

Para produzir uma conversa de verdade no frontend, basta reutilizar o `conversation_id` retornado e exibir o array `messages` em formato de chat.

## Como rodar (dev)

```bash
python -m venv .venv
".venv\\Scripts\\activate"  # PowerShell Windows
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

### DocumentaÃ§Ã£o OpenAPI/Swagger

O FastAPI gera automaticamente a documentaÃ§Ã£o dos endpoints em formato OpenAPI. Para visualizar e testar os endpoints, basta acessar:

- [http://localhost:8000/docs](http://localhost:8000/docs) â€” Interface interativa Swagger UI
- [http://localhost:8000/openapi.json](http://localhost:8000/openapi.json) â€” Arquivo OpenAPI em JSON

Esses caminhos funcionam por padrÃ£o ao rodar o backend com FastAPI/Uvicorn. NÃ£o Ã© necessÃ¡rio configuraÃ§Ã£o extra.

> [FastAPI](https://fastapi.tiangolo.com/)  
> [Swagger](https://swagger.io/)

## ğŸ“¦ Stack Local (Embeddings + Rerank)

Fluxo tÃ­pico para adicionar uma nova lei usando somente recursos locais:

```bash
# 1. IngestÃ£o (gera JSONL processado de artigos/chunks)
python -m scripts.ingest --lei "11.101/2005" --input data/raw/lei_11101_2005.txt

# (Opcional) baixar via URL oficial:
python -m scripts.ingest --lei "11.101/2005" --url "https://www.planalto.gov.br/..." --output data/processed/lei_11101_2005.jsonl --raw-html-out data/raw/lei_11101_2005.html

# 2. IndexaÃ§Ã£o local (embeddings CPU)
python -m scripts.index_qdrant_local --jsonl data/processed/lei_11101_2005.jsonl --collection leis --recreate

# 3. Busca vetorial simples
python -m scripts.search_qdrant_local --query "plano de recuperaÃ§Ã£o judicial" --k 8

# 4. Busca + rerank (melhor precisÃ£o)
python -m scripts.search_qdrant_local --query "plano de recuperaÃ§Ã£o judicial" --k 12 --n 5 --rerank
```

VariÃ¡veis Ãºteis:

```text
EMBED_MODEL=intfloat/multilingual-e5-base   # mudar modelo de embeddings
RERANK_MODEL=BAAI/bge-reranker-v2-m3        # mudar modelo cross-encoder
QDRANT_COLLECTION=leis                      # nome da collection
QDRANT_HOST=localhost QDRANT_PORT=6333      # endpoint Qdrant
```

Para ver todos os resultados antes do rerank final: `--show-all`.

## ğŸ§  GeraÃ§Ã£o de Documento com IA (PetiÃ§Ã£o Inicial de CobranÃ§a)

AlÃ©m de preencher manualmente os campos do JSON para o endpoint de documento, vocÃª pode gerar seÃ§Ãµes automaticamente (fatos, pedidos, provas) usando recuperaÃ§Ã£o + LLM local (Ollama).

### PrÃ©-requisitos

1. Qdrant rodando local e jÃ¡ indexado (ver seÃ§Ã£o anterior).
2. Ollama instalado e modelo carregado (ex.: `ollama pull llama3.1:8b`).
3. VariÃ¡veis de ambiente (opcionais):
   - `OLLAMA_HOST` (default `http://localhost:11434`)
   - `OLLAMA_MODEL` (ex.: `llama3.1:8b`)

### FunÃ§Ã£o Python

A funÃ§Ã£o `generate_peticao_inicial_cobranca_ai` em `app/documents/generator.py`:

- Recupera artigos relevantes da coleÃ§Ã£o (embeddings locais)
- Gera texto estruturado para fatos / pedidos / provas se estiverem vazios ou se `force=True`
- Renderiza o template `.docx` final

```python
from app.documents.generator import generate_peticao_inicial_cobranca_ai

entrada = {
  "foro": "Foro Central da Comarca X",
  "autor": {"nome": "JoÃ£o Silva", "cpf": "123.456.789-00", "endereco": "Rua A, 100"},
  "reu": {"nome": "Empresa Y Ltda.", "cnpj": "12.345.678/0001-99", "endereco": "Av. B, 200"},
  "valor_causa": 15000.00,
  # campos fatos/pedidos/provas vazios => serÃ£o gerados
}

doc_path = generate_peticao_inicial_cobranca_ai(
  entrada,
  consulta_caso="Cliente nÃ£o recebeu valores de contrato de prestaÃ§Ã£o de serviÃ§os firmado em 2023.",
  k=12,            # nÃºmero de chunks recuperados
  n_context=6,     # reservado para futura lÃ³gica (rerank)
  force=False      # True para sobrescrever se jÃ¡ houver conteÃºdo
)
print("Gerado:", doc_path)
```

### Como funciona internamente

1. Normaliza a descriÃ§Ã£o do caso com `preprocess_question`.
2. Busca vetorial em Qdrant (`k` resultados).
3. Monta o CONTEXTO concatenando trechos (truncados para ~900 chars cada).
4. Chama o Ollama usando um prompt jurÃ­dico padronizado (cita artigos se possÃ­vel).
5. Para `pedidos` e `provas`, transforma a resposta em lista de itens por linha.
6. Renderiza docx final com `docxtpl`.

### Dicas de Prompt

- ForneÃ§a contexto factual claro em `consulta_caso`.
- Ajuste `k` se vier pouco fundamento legal (maior recall).
- Se a saÃ­da vier prolixa, considere reduzir o modelo ou pÃ³s-processar (ex.: limitar nÃºmero de linhas em pedidos).

### PossÃ­veis ExtensÃµes

- Reranqueamento dos artigos antes da geraÃ§Ã£o (usar `rerank_local.py`).
- GeraÃ§Ã£o de fundamentaÃ§Ã£o jurÃ­dica e jurisprudÃªncia em seÃ§Ãµes separadas.
- VerificaÃ§Ã£o automÃ¡tica de citaÃ§Ãµes legais (regex para "Art.").

## Arquitetura: Frontend, Backend e ServiÃ§o de HistÃ³rico

### O sistema Ã© composto por trÃªs principais componentes

1. **Frontend**
Interface web utilizada pelo usuÃ¡rio para enviar perguntas jurÃ­dicas, visualizar respostas e acompanhar o histÃ³rico de conversas.

2. **Backend (Legal Assistant)**
API central que recebe as requisiÃ§Ãµes do frontend, processa perguntas, realiza busca semÃ¢ntica, gera respostas fundamentadas e interage com o serviÃ§o de histÃ³rico para registrar e recuperar conversas.

3. **ServiÃ§o de HistÃ³rico**
MicroserviÃ§o responsÃ¡vel por armazenar e recuperar o histÃ³rico das conversas dos usuÃ¡rios, permitindo persistÃªncia e contexto em mÃºltiplos turnos.

Fluxo de InteraÃ§Ã£o
O usuÃ¡rio interage com o frontend, enviando perguntas ou comandos.
O frontend faz requisiÃ§Ãµes HTTP para o backend (ex: `/chat`, `/conversation/{id}`).
O backend processa a requisiÃ§Ã£o, consulta o serviÃ§o de histÃ³rico para obter ou atualizar o contexto da conversa.
O backend realiza busca semÃ¢ntica, gera resposta (usando RAG + LLM local) e retorna ao frontend.
O frontend exibe a resposta e o histÃ³rico atualizado ao usuÃ¡rio.

```mermaid
sequenceDiagram
    participant UsuÃ¡rio
    participant Frontend
    participant Backend
    participant HistÃ³rico

    UsuÃ¡rio->>Frontend: Envia pergunta ou comando
    Frontend->>Backend: POST /chat (com conversation_id)
    Backend->>HistÃ³rico: GET/POST conversa (recupera ou atualiza histÃ³rico)
    HistÃ³rico-->>Backend: Retorna histÃ³rico da conversa
    Backend->>Backend: Processa pergunta, busca contexto, gera resposta
    Backend->>HistÃ³rico: POST mensagem (atualiza histÃ³rico)
    Backend-->>Frontend: Retorna resposta e histÃ³rico
    Frontend-->>UsuÃ¡rio: Exibe resposta e histÃ³rico
```

[VÃ­deo demonstrativo](https://youtu.be/FMrRUNGxe1Y)
